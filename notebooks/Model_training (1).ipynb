{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iVZWFI3tSySC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mqVdn6ETSP-",
        "outputId": "e237fc2c-943a-4f3f-882a-dc9b46f56e11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"../\")\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XpVwrM7WTVBQ",
        "outputId": "85f5852c-8d54-4d76-cfe3-86d16927c091"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('preprocessed_train_data.csv')\n",
        "test_data=pd.read_csv('preprocessed_test_data.csv')"
      ],
      "metadata": {
        "id": "KK6gtXOGTi7a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x=train_data.drop(['Item_Outlet_Sales'],axis=1)\n",
        "train_y=train_data['Item_Outlet_Sales']\n",
        "test_x=test_data.drop(['Item_Outlet_Sales'],axis=1)\n",
        "test_y=test_data['Item_Outlet_Sales']"
      ],
      "metadata": {
        "id": "1Kp9dVbNTnWh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RandomForestRegressor\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at each leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider for best split\n",
        "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(train_x, train_y)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test data\n",
        "test_rmse = np.sqrt(-grid_search.score(test_x,test_y))  # RMSE from neg_mean_squared_error scoring\n",
        "print(f'Test RMSE: {test_rmse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELWlmDq2UQlO",
        "outputId": "bbd0f79a-029a-4af1-fbe5-3e3110936afb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "1080 fits failed out of a total of 3240.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "405 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "675 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [              nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan -1305330.76324504\n",
            " -1288198.30335201 -1275972.76273069 -1291980.67591695 -1275463.32052706\n",
            " -1268701.5256073  -1280020.50569353 -1271782.07977662 -1274380.96578586\n",
            " -1279518.07353029 -1275014.6499643  -1269218.65809032 -1275535.07542093\n",
            " -1269013.82958124 -1266153.13825786 -1278902.5717257  -1273030.78689194\n",
            " -1275037.20319068 -1283725.38836919 -1279461.18384828 -1283910.67487549\n",
            " -1290710.62964364 -1290288.02433926 -1286069.67167195 -1295139.24676002\n",
            " -1275900.26002693 -1285012.98032227 -1313174.77728225 -1283582.25485092\n",
            " -1283419.91836927 -1283466.55672214 -1277314.89320227 -1269148.57838447\n",
            " -1276283.91911074 -1267048.5968033  -1267838.31787374 -1277716.35329886\n",
            " -1274620.28490855 -1271271.8305776  -1278181.54395421 -1275027.69196866\n",
            " -1270123.57235083 -1289372.06753277 -1277522.96843092 -1270204.33386836\n",
            " -1287150.26302735 -1286218.28521446 -1287167.2490188  -1294142.96868053\n",
            " -1289695.07439578 -1284458.49658048 -1290456.52150836 -1299093.10780323\n",
            " -1287210.6438258                nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            " -1349358.61171996 -1345323.62221373 -1348615.77275937 -1343475.82446607\n",
            " -1354096.93702616 -1351961.18503661 -1363751.12855046 -1363726.84084075\n",
            " -1348852.55493778 -1377549.63209969 -1359644.99105653 -1346650.4891859\n",
            " -1358235.45520206 -1351855.68371688 -1348552.58157899 -1376713.00037457\n",
            " -1365522.07636074 -1354794.29171143 -1378593.23338028 -1358667.91243335\n",
            " -1361698.99363139 -1397013.24600605 -1361251.44987701 -1370063.96091889\n",
            " -1363044.42384027 -1363135.95579004 -1364630.21580594 -1357053.10418637\n",
            " -1351520.53246598 -1351148.96212451 -1358441.83096574 -1350864.10836715\n",
            " -1346113.81641972 -1349723.44002727 -1343147.72853912 -1357848.81527276\n",
            " -1352883.158743   -1352073.53994587 -1343172.06101389 -1368349.70819292\n",
            " -1351528.41027208 -1357123.14668474 -1367615.99800345 -1361879.33321623\n",
            " -1355795.19194502 -1357936.28674976 -1369348.01731393 -1367975.26700593\n",
            " -1381908.40175556 -1366071.97246736 -1360233.91224606 -1393481.32392558\n",
            " -1362801.42516587 -1370829.50066494               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan -1296771.9450873  -1278266.73827571 -1277351.37778996\n",
            " -1291465.75989081 -1277916.4853937  -1271282.02666947 -1287979.54872496\n",
            " -1272822.75816106 -1275507.40532902 -1274032.76212668 -1279452.82291355\n",
            " -1264903.53748868 -1280027.272703   -1273091.50579006 -1270721.06215044\n",
            " -1277753.17301396 -1277204.60411926 -1270704.93974269 -1311441.14834722\n",
            " -1285738.07810127 -1287434.60701939 -1287806.28435892 -1292478.63967071\n",
            " -1287933.27800308 -1294182.99599117 -1295702.39273498 -1284021.1921\n",
            " -1288599.20771612 -1282073.23361898 -1277373.9987973  -1291654.81412974\n",
            " -1272041.30146358 -1271101.96413546 -1282205.64549438 -1276656.79753848\n",
            " -1268459.70188152 -1288630.95918432 -1275861.4789948  -1270258.07957044\n",
            " -1288556.1663918  -1283131.30751653 -1273508.61501675 -1281543.46962061\n",
            " -1275320.53018783 -1272620.9865303  -1288465.35298192 -1288611.09698091\n",
            " -1282824.59019381 -1283669.60206906 -1289482.17392102 -1290540.70160272\n",
            " -1302823.46940195 -1287742.22389576 -1287334.957387                 nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan -1305330.45331919 -1281789.82305768\n",
            " -1287953.31342633 -1283393.16340822 -1272183.11270376 -1272106.17018492\n",
            " -1282984.09393657 -1275397.68406325 -1271597.33039743 -1290789.39904636\n",
            " -1270630.60568001 -1270424.33324077 -1283433.33422552 -1273196.66434808\n",
            " -1266313.56709655 -1281540.3529844  -1271461.6499277  -1268258.43071978\n",
            " -1291734.62208008 -1287117.67643321 -1282369.27069545 -1288059.44615725\n",
            " -1281492.55307348 -1287135.054934   -1291913.66899469 -1291056.98617407\n",
            " -1287689.98724115 -1299055.35333583 -1296064.70616333 -1277747.88418665\n",
            " -1289357.67047075 -1269527.74167335 -1267260.03958289 -1279094.72922611\n",
            " -1268951.75876028 -1269817.94576716 -1283151.55425924 -1264392.91403941\n",
            " -1273727.01705114 -1286864.90010422 -1272175.31603947 -1275059.64733821\n",
            " -1300274.65402788 -1274654.31977136 -1271224.47073681 -1295991.91207329\n",
            " -1285699.88124432 -1280171.82633107 -1301163.96362944 -1283885.07159177\n",
            " -1285714.80609922 -1299914.97871259 -1293386.34678934 -1288176.58046279\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan -1335733.6659317\n",
            " -1321231.47138222 -1309313.69107903 -1302944.58217125 -1292128.94203826\n",
            " -1290013.07415703 -1300834.97268894 -1281463.49210965 -1278783.55986767\n",
            " -1297219.0298102  -1287350.96857713 -1277982.77131793 -1289700.92110893\n",
            " -1286502.27555925 -1278203.4001416  -1280624.13640675 -1277226.36185647\n",
            " -1273981.60174088 -1291016.26035514 -1280798.64579682 -1273267.18565669\n",
            " -1282275.79046284 -1282793.94177161 -1274193.89252529 -1278049.40414845\n",
            " -1281553.17490695 -1277681.27490659 -1338985.37283847 -1320840.74161174\n",
            " -1315075.29786136 -1321084.25704208 -1299427.76178119 -1298634.24230091\n",
            " -1298093.63589286 -1284704.234876   -1277369.19309227 -1292934.43394556\n",
            " -1279994.54887273 -1281648.14402414 -1294266.77013514 -1284892.46328595\n",
            " -1279796.19492223 -1277415.91620921 -1273424.78931236 -1279219.3868216\n",
            " -1285271.85355955 -1272331.5018147  -1277271.27362947 -1285178.84123466\n",
            " -1278335.93153086 -1274148.68505809 -1277546.86647216 -1279660.62757248\n",
            " -1276529.27375557               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            " -1347989.92821539 -1347924.82064206 -1354440.49685428 -1356704.31618531\n",
            " -1356522.33603037 -1348684.5708501  -1358463.97191785 -1356441.05813876\n",
            " -1353073.79747267 -1381527.45056072 -1350192.16896107 -1351529.48498039\n",
            " -1365100.16400023 -1351480.03957916 -1350165.68703484 -1362840.15843988\n",
            " -1359166.19943323 -1363104.74918515 -1369014.62581481 -1356971.94196477\n",
            " -1363097.46073236 -1379323.3450501  -1363743.52009068 -1361495.98732742\n",
            " -1366043.52692284 -1362404.00196433 -1368187.16714793 -1357450.85505578\n",
            " -1373589.35512083 -1356221.72522915 -1370780.57831144 -1355945.63284683\n",
            " -1347037.58142735 -1354111.97093139 -1352070.1603222  -1355245.51975612\n",
            " -1355035.54245917 -1348454.30074997 -1359514.33332066 -1353920.42805915\n",
            " -1342524.82332169 -1348870.93536739 -1379057.14811382 -1355788.49849076\n",
            " -1353975.85354856 -1372306.28983253 -1367703.02519655 -1362624.76682908\n",
            " -1355919.24978333 -1367613.571214   -1357119.84546866 -1344865.6142411\n",
            " -1364686.8109072  -1356509.59635825               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan -1314368.02666706 -1299450.19529789 -1294022.08021514\n",
            " -1295998.72186254 -1291413.81402125 -1288285.72236931 -1282802.00575029\n",
            " -1279857.6078083  -1280520.97053203 -1290409.44867212 -1280873.19003924\n",
            " -1281191.99430313 -1289648.51748883 -1279858.16385546 -1275117.08260707\n",
            " -1281979.93903085 -1275654.76141014 -1277710.94891235 -1276478.28268277\n",
            " -1274814.70738681 -1277058.73790136 -1291417.47937739 -1282561.075049\n",
            " -1274982.31951474 -1283896.52215151 -1283063.83374241 -1276561.48948127\n",
            " -1319495.54906589 -1295682.95548353 -1299850.52897849 -1304380.31629719\n",
            " -1292683.95118516 -1287696.50521185 -1277606.73632836 -1284246.244609\n",
            " -1275458.57887681 -1295337.86974284 -1272152.93046401 -1279220.99433671\n",
            " -1284440.03750633 -1280421.0219293  -1273989.50908929 -1284411.67991656\n",
            " -1279473.36926727 -1277751.51705506 -1274399.38751003 -1281775.51208812\n",
            " -1280743.35117288 -1292724.64911908 -1277982.85653686 -1274577.04712296\n",
            " -1289150.18185256 -1279260.91073925 -1273723.08374077               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan -1329038.44107536 -1321130.7520153\n",
            " -1309454.638899   -1316425.16414229 -1301902.43064357 -1298025.46363458\n",
            " -1292014.54514473 -1286381.97982937 -1277381.1693137  -1291458.05836318\n",
            " -1284305.12921582 -1279470.16973707 -1285746.52417661 -1288570.45718953\n",
            " -1280749.78948396 -1288358.87532384 -1276234.33064704 -1273240.63183362\n",
            " -1282663.31704149 -1279938.04897257 -1273183.51633211 -1279931.10132032\n",
            " -1280840.39842082 -1275519.84280921 -1288460.0094773  -1274173.87198569\n",
            " -1278288.98343549 -1331499.20506689 -1316325.29943068 -1312568.86951416\n",
            " -1320238.14078905 -1294870.29502261 -1294833.18588936 -1300598.41836852\n",
            " -1281301.93481101 -1279465.38964416 -1301525.18897262 -1293697.73063772\n",
            " -1281333.02008156 -1294241.88288143 -1281790.99307636 -1283565.35092961\n",
            " -1282593.64847821 -1273279.97554769 -1274705.04124232 -1286352.15179005\n",
            " -1276104.02266614 -1277209.76724745 -1290042.44467521 -1279056.56109951\n",
            " -1279713.8171794  -1290645.27854671 -1280547.07372041 -1274226.75483875]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'bootstrap': True, 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Test RMSE: 1049.9310562260557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import  cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error,r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAhbnjt1pCDs",
        "outputId": "bf62f5d8-4014-4aad-ed71-31e504d66462"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GradientBoostingRegressor model\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "# Define the hyperparameter grid to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 200, 250],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 5, 7, 9, 11],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
        "    'loss': ['ls', 'huber', 'absolute_error']\n",
        "}\n",
        "# Set up randomSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=gbr, param_distributions=param_grid,n_iter=100, cv=5, verbose=2, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit random search\n",
        "random_search.fit(train_x, train_y)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# Evaluate on test data\n",
        "test_rmse = np.sqrt(-random_search.score(test_x,test_y))  # RMSE from neg_mean_squared_error scoring\n",
        "print(f'Test RMSE: {test_rmse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84QU9CtBeG5J",
        "outputId": "dd18645c-2000-4984-d773-d929bfb8b916"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "260 fits failed out of a total of 500.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "153 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingRegressor must be a str among {'absolute_error', 'quantile', 'squared_error', 'huber'}. Got 'ls' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "52 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingRegressor must be a str among {'huber', 'quantile', 'squared_error', 'absolute_error'}. Got 'ls' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "55 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [              nan               nan -1282756.71227096 -1235142.62475281\n",
            " -1454682.61219193               nan -1265773.44374262               nan\n",
            "               nan -1441321.6578641  -1562280.70215578               nan\n",
            " -1394233.59468378 -1381255.90400132 -1456708.52811073 -1299087.229958\n",
            " -1284071.03993379 -1399693.39591012 -1879980.61263498 -1280843.57641618\n",
            " -1316050.62816693 -1365149.38024214 -1342720.1099323                nan\n",
            "               nan -1297877.56513125               nan -1325901.79462252\n",
            "               nan -1925185.848407   -1308925.64227466 -1555712.68067323\n",
            " -1344849.51679064 -1253751.88654071               nan -1470278.29881468\n",
            "               nan -2337799.10518525 -1278780.53274945 -1305132.36237185\n",
            "               nan               nan               nan               nan\n",
            "               nan               nan -1462296.83041626               nan\n",
            " -1292934.00911446               nan -1354646.48098069               nan\n",
            "               nan               nan               nan               nan\n",
            " -1230358.51743935               nan               nan               nan\n",
            "               nan -1288877.37179619               nan               nan\n",
            " -1452494.11188576 -1940973.73113423 -2172791.08624266 -1363050.68562497\n",
            "               nan               nan -1380255.8374514  -1244564.58938478\n",
            "               nan               nan               nan               nan\n",
            " -1326692.08016889               nan               nan               nan\n",
            "               nan               nan -1407595.20252607               nan\n",
            " -1581721.66190653 -1323369.74596344 -1357543.31170735 -1276653.43824991\n",
            "               nan -2663983.23785471               nan -1751191.77939868\n",
            " -1279427.20483529               nan               nan               nan\n",
            "               nan               nan               nan               nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 250, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None, 'max_depth': 3, 'loss': 'absolute_error', 'learning_rate': 0.1}\n",
            "Test RMSE: 1027.9806413263875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize GradientBoostingRegressor with the specified hyperparameters\n",
        "gbr = GradientBoostingRegressor(\n",
        "    subsample=0.8,              # Fraction of samples to be used for fitting each base learner\n",
        "    n_estimators=250,           # Number of boosting stages to be used\n",
        "    min_samples_split=10,       # Minimum number of samples required to split an internal node\n",
        "    min_samples_leaf=5,         # Minimum number of samples required to be at a leaf node\n",
        "    max_features=None,          # Number of features to consider for the best split\n",
        "    max_depth=3,                # Maximum depth of the individual trees\n",
        "    loss='absolute_error',      # Loss function to optimize\n",
        "    learning_rate=0.1          # Step size shrinking to improve model generalization\n",
        ")\n",
        "\n",
        "# Fit the model on the training data\n",
        "gbr.fit(train_x, train_y)\n",
        "\n",
        "# Predict on the training data\n",
        "predict_value_train = gbr.predict(train_x)\n",
        "\n",
        "# Calculate RMSE and R² on the training data\n",
        "train_rmse = np.sqrt(mean_squared_error(train_y, predict_value_train))\n",
        "train_r2 = r2_score(train_y, predict_value_train)\n",
        "\n",
        "# Output training results\n",
        "print(f'Train RMSE: {train_rmse}')\n",
        "print(f'Train R²: {train_r2}')\n",
        "\n",
        "# Predict on the test data\n",
        "test_predict = gbr.predict(test_x)\n",
        "\n",
        "# Calculate RMSE and R² on the test data (assuming you have test_y as the true values)\n",
        "test_rmse = np.sqrt(mean_squared_error(test_y, test_predict))\n",
        "test_r2 = r2_score(test_y, test_predict)\n",
        "\n",
        "# Output test results\n",
        "print(f'Test RMSE: {test_rmse}')\n",
        "print(f'Test R²: {test_r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6DEIb9xxXyy",
        "outputId": "3e2aac03-96fe-4ac2-9c6e-37dfc04380dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 1057.9738759808943\n",
            "Train R²: 0.6216059116660508\n",
            "Test RMSE: 1029.1322550937346\n",
            "Test R²: 0.6103293367786498\n"
          ]
        }
      ]
    }
  ]
}